{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Development, Validation and Comparison\n",
    "\n",
    "Keywords: baseline, load forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of Python library Pandas and Numpy to clean a set of building meter data and weather data for baseline model development and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Energy Baseline Model\n",
    "This notebook demonstrates the process of processing, and analyzing building energy data and weather data to generate baseline model and model performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Datasets\n",
    "We will load the meter and weather data, as well as any supporting data (e.g., holidays, event days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/6vcnqd4d3nlcwtdmsgb31hx40000gn/T/ipykernel_52467/2890274037.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime           site_id  power\n",
      "0 2008-06-01 00:00:00  TwoCarnegiePlaza  36.00\n",
      "1 2008-06-01 00:15:00  TwoCarnegiePlaza  37.44\n",
      "2 2008-06-01 00:30:00  TwoCarnegiePlaza  37.92\n",
      "3 2008-06-01 00:45:00  TwoCarnegiePlaza  37.44\n",
      "4 2008-06-01 01:00:00  TwoCarnegiePlaza  37.44\n",
      "          time  apparentTemperature  cloudCover  dewPoint  humidity  \\\n",
      "0  1/1/08 0:00                 8.22         0.0  -10.5800      0.24   \n",
      "1  1/1/08 0:15                 8.34         0.0  -10.6125      0.24   \n",
      "2  1/1/08 0:30                 8.46         0.0  -10.6450      0.24   \n",
      "3  1/1/08 0:45                 8.58         0.0  -10.6775      0.24   \n",
      "4  1/1/08 1:00                 8.70         0.0  -10.7100      0.24   \n",
      "\n",
      "          icon  precipIntensity  precipProbability precipType   pressure  \\\n",
      "0  clear-night              0.0                0.0        NaN  1024.1900   \n",
      "1          NaN              0.0                0.0        NaN  1024.0975   \n",
      "2          NaN              0.0                0.0        NaN  1024.0050   \n",
      "3          NaN              0.0                0.0        NaN  1023.9125   \n",
      "4  clear-night              0.0                0.0        NaN  1023.8200   \n",
      "\n",
      "  summary  temperature  uvIndex  visibility  windBearing  windGust  windSpeed  \\\n",
      "0   Clear         8.71      0.0      16.089        80.00     3.780     1.4600   \n",
      "1     NaN         8.79      0.0      16.089        73.25     3.655     1.4375   \n",
      "2     NaN         8.87      0.0      16.089        66.50     3.530     1.4150   \n",
      "3     NaN         8.95      0.0      16.089        59.75     3.405     1.3925   \n",
      "4   Clear         9.03      0.0      16.089        53.00     3.280     1.3700   \n",
      "\n",
      "             datetime  \n",
      "0 2008-01-01 00:00:00  \n",
      "1 2008-01-01 00:15:00  \n",
      "2 2008-01-01 00:30:00  \n",
      "3 2008-01-01 00:45:00  \n",
      "4 2008-01-01 01:00:00  \n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "meter_data_path = 'data/chapter2/meter-data/TwoCarnegiePlaza.csv'\n",
    "weather_data_path = 'data/chapter2/SanBernadino_2018-01-01_2020-01-01_Weather.csv'\n",
    "\n",
    "# Load meter and weather data\n",
    "meter_data = pd.read_csv(meter_data_path)\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "\n",
    "# Convert datetime columns\n",
    "meter_data['datetime'] = pd.to_datetime(meter_data['datetime'], format='%m/%d/%y %H:%M')\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n",
    "\n",
    "# Inspect data\n",
    "print(meter_data.head())\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Baseline Models\n",
    "The `BaselineModels` class provides methods for calculating baseline energy usage and evaluating demand response performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModels(object):\n",
    "    def __init__(self, site, input_data, event):\n",
    "        self.site = site\n",
    "        self.data = input_data\n",
    "\n",
    "        # event day\n",
    "        self.event = event\n",
    "        self.event_start = datetime.strftime(datetime.strptime(\n",
    "            event['shed_start_time'], '%m/%d/%y %H:%M'), '%H:%M')\n",
    "        self.shed_start = datetime.strftime(datetime.strptime(event['shed_start_time'], '%m/%d/%y %H:%M') + timedelta(minutes=15),\n",
    "                                            '%H:%M')\n",
    "        self.shed_end = datetime.strftime(datetime.strptime(\n",
    "            event['shed_end_time'], '%m/%d/%y %H:%M'), '%H:%M')\n",
    "\n",
    "    def get_x_baseline_days(self, x):\n",
    "        train_start_date = datetime.strftime(datetime.strptime(self.event['date'], '%Y-%m-%d') - timedelta(days=x),\n",
    "                                             '%Y/%m/%d')\n",
    "        train_end_date = datetime.strftime(datetime.strptime(self.event['date'], '%Y-%m-%d') - timedelta(days=1),\n",
    "                                           '%Y/%m/%d')\n",
    "        train_data = self.data[self.data['valid_dates']\n",
    "                               == 1][train_start_date:train_end_date]\n",
    "        if train_data.empty:\n",
    "            print('training baseline data is empty')\n",
    "        else:\n",
    "            print('training baseline data is not empty')\n",
    "        # print(train_data.head())\n",
    "        return train_data\n",
    "\n",
    "    def calc_x_y_baseline_days(self, x, y):\n",
    "        # event type and data_day timestamp\n",
    "        if x > y:\n",
    "            print(\"Input of 'x' should be equal or less than 'y'.\")\n",
    "        # event_date = self.event.date\n",
    "        # data_wk = self.data[(self.data['valid_dates'] == 1)]\n",
    "        train_data = self.get_x_baseline_days(x=30)\n",
    "        baseline_y_days = sorted(set(train_data.day))[-y:]\n",
    "        baseline_y_data = train_data[train_data['day'].isin(baseline_y_days)]\n",
    "        # calculate and sort daily peak power\n",
    "        selected_x_days = baseline_y_data.groupby(\n",
    "            'day')['power'].max().sort_values()[-x:].index.tolist()\n",
    "        selected_x_data = baseline_y_data[baseline_y_data['day'].isin(\n",
    "            selected_x_days)]\n",
    "\n",
    "        return selected_x_data\n",
    "\n",
    "    def calc_match_baseline(self, x, y):\n",
    "        # get the matching baseline day from previous 10 valid baseline days\n",
    "        baseline_data = self.calc_x_y_baseline_days(x, y)\n",
    "        # calculate avg baseline\n",
    "        dr_data = self.data[self.event.date][[\n",
    "            'time', 'day', 'hour', 'oat', 'power']].copy()\n",
    "        oa_sum_diff = pd.Series(index=list(set(baseline_data.day)))\n",
    "        for i in baseline_data.day.unique():\n",
    "            #             print(baseline_data[i].oat.values-dr_data.oat.values)\n",
    "            try:\n",
    "                oa_sum_diff[i] = np.sum(\n",
    "                    np.square(baseline_data[i].oat.values - dr_data.oat.values), axis=0)\n",
    "            except ValueError:\n",
    "                print('Missing OA data on {}'.format(i))\n",
    "        match_day = oa_sum_diff.idxmin()\n",
    "        try:\n",
    "            dr_data['baseline'] = baseline_data[match_day].power.values\n",
    "        except KeyError:\n",
    "            print(\"Data is missing on {}\".format(self.event.date))\n",
    "            pass\n",
    "        return dr_data[['oat', 'baseline', 'power']]\n",
    "\n",
    "    def calc_avg_baseline_adj(self, x, y, adj_limit):\n",
    "        # event\n",
    "        adj_start = datetime.strftime(datetime.strptime(\n",
    "            self.event_start, '%H:%M') + timedelta(hours=-5), '%H:%M')\n",
    "        adj_end = datetime.strftime(datetime.strptime(\n",
    "            self.event_start, '%H:%M') + timedelta(hours=-2), '%H:%M')\n",
    "        # get baseline days\n",
    "        baseline_data = self.calc_x_y_baseline_days(x, y)\n",
    "        # calculate avg baseline\n",
    "        dr_data = self.data[self.event['date']][[\n",
    "            'time', 'day', 'hour', 'oat', 'power']].copy()\n",
    "        # dr_data.index = dr_data.time\n",
    "        dr_data['avg_baseline'] = baseline_data.groupby('time')['power'].mean()\n",
    "        try:\n",
    "            dr_data.loc[self.shed_start:self.shed_end,\n",
    "                        ['event_hours']] = 1\n",
    "            # calculate avg model adjustment factor\n",
    "            adj_factor = np.mean(\n",
    "                dr_data[adj_start:adj_end]['power'] / dr_data[adj_start:adj_end]['avg_baseline'])\n",
    "            if adj_factor > (1 + adj_limit):\n",
    "                adj_factor = (1 + adj_limit)\n",
    "            elif adj_factor < (1 - adj_limit):\n",
    "                adj_factor = (1 - adj_limit)\n",
    "            dr_data['baseline'] = dr_data['avg_baseline'] * adj_factor\n",
    "        except KeyError:\n",
    "            print(\"Data is missing on {}\".format(self.event.date))\n",
    "            pass\n",
    "        return dr_data[['oat', 'baseline', 'power']]\n",
    "\n",
    "    def oat_reg_model(data):\n",
    "        data['hr'] = data['hour'].astype(str)\n",
    "        X = data[['oat', 'hr']]\n",
    "        X_hr = pd.get_dummies(X)\n",
    "        y_hr = data['power']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_hr, y_hr,\n",
    "                                                            test_size=0.2, random_state=101)\n",
    "        \n",
    "        # train the regression model\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        # output model\n",
    "        coeff_parameter = pd.DataFrame(model.coef_, X.columns,columns=['Coefficient'])\n",
    "        print('Intercept: \\n', model.intercept_)\n",
    "        print('Coefficients: \\n', model.coef_)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define SiteData Class\n",
    "The `SiteData` class processes site-specific information, including holiday schedules, DR events, and weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteData(object):\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "\n",
    "        # Create sublevel directories\n",
    "        self.root_dir = pathlib.Path.cwd()\n",
    "        self.data_dir = self.root_dir.joinpath('data/chapter2')\n",
    "        self.out_dir = self.root_dir.joinpath('data/chapter2/{}'.format(site))\n",
    "        self.plot_dir = self.root_dir.joinpath(\n",
    "            'figures/chapter2/{}'.format(site['site_id']))\n",
    "\n",
    "        # Create directories\n",
    "        for dir_inst in [self.out_dir, self.plot_dir]:\n",
    "            try:\n",
    "                if not os.path.exists(dir_inst):\n",
    "                    os.makedirs(dir_inst)\n",
    "            except FileExistsError:\n",
    "                continue\n",
    "        # read meter, weather and event data\n",
    "        self.holidays = self.read_special_days()\n",
    "        self.event_days = self.read_event_days()\n",
    "\n",
    "        # read meter and weather data\n",
    "        self.data = self.read_site_data()\n",
    "\n",
    "    def read_special_days(self):\n",
    "        holidays = pd.read_csv(\n",
    "            self.root_dir.joinpath(self.data_dir.joinpath('special-days.csv')))\n",
    "        holidays['date'] = pd.to_datetime(\n",
    "            pd.Series(holidays['date']), format='%m/%d/%y')\n",
    "        holidays['day'] = holidays.date.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        return holidays\n",
    "\n",
    "    def read_event_days(self):\n",
    "        dr_event = pd.read_csv(self.data_dir.joinpath('dr-event.csv'))\n",
    "        dr_event['event_date'] = dr_event.event_date.apply(\n",
    "            lambda x: datetime.strptime(x, '%m/%d/%y'))\n",
    "        dr_event['date'] = dr_event.event_date.apply(\n",
    "            lambda x: x.strftime('%Y-%m-%d'))\n",
    "        return dr_event\n",
    "\n",
    "    def read_site_data(self):\n",
    "        df = pd.read_csv('data/chapter2/final-data.csv', index_col=[0], parse_dates=True)\n",
    "        # fill missing data or na\n",
    "        df = df.ffill()\n",
    "        # remove duplicated index\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "        # convert temperature to fahrenheit\n",
    "        df['oat'] = df.temperature*1.8+32\n",
    "\n",
    "        # clean dataframe\n",
    "        df['date'] = df.index\n",
    "        df['year'] = df.date.apply(lambda x: x.strftime('%Y'))\n",
    "        df['time'] = df.date.apply(lambda x: x.strftime('%H:%M'))\n",
    "        # df['month'] = df.date.apply(lambda x: int(x.strftime('%m')))\n",
    "        df['day'] = df.date.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        df['hour'] = df.date.apply(lambda x: int(x.strftime('%H')))\n",
    "        # df['minute'] = df.date.apply(lambda x: x.strftime('%M'))\n",
    "        df['weekday'] = df.date.apply(lambda x: int(x.strftime('%w')))\n",
    "        df['DR'] = df.date.apply(\n",
    "            lambda x: x.strftime('%Y-%m-%d') in self.event_days.date.values)\n",
    "        df['DR'] = df.DR.astype(int)\n",
    "        df['holiday'] = df.date.apply(lambda x: x.strftime(\n",
    "            '%Y-%m-%d') in self.holidays.day.values)\n",
    "        df['holiday'] = df.holiday.astype(int)\n",
    "        df['valid_dates'] = 0\n",
    "        df.loc[(df['weekday'] > 0) & (df['weekday'] < 6) & (\n",
    "            df['holiday'] == 0) & (df['DR'] == 0), ['valid_dates']] = 1\n",
    "        print('read the preprocessed meter and weather data.')\n",
    "        return df\n",
    "\n",
    "    # read weather data if needed\n",
    "    def read_weather_data(self):\n",
    "        df = pd.read_csv(self.data_dir.joinpath('final-data.csv'),\n",
    "                         index_col=[0], parse_dates=True)\n",
    "        # df.index = pd.to_datetime(df.datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "        df['oat'] = df.temperature*1.8+32\n",
    "        df= df.ffill()\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "        # resample to 15 minutes\n",
    "        df = df.resample('15min').interpolate(method='linear')\n",
    "        print('read the weather data.')\n",
    "        # print(df.head())\n",
    "        return df['oat']\n",
    "\n",
    "    def calc_load_stats(self):\n",
    "        test2 = self.data[(self.data['year'] == '2008')][[\n",
    "            'power', 'oat', 'month', 'hour', 'weekday']].resample('1h').mean()\n",
    "        df_wk = test2[(test2.hour > 6) & (test2.hour < 23) &\n",
    "                      (test2.weekday > 0) & (test2.weekday < 7)]\n",
    "        df_wkd = test2[~((test2.hour > 6) & (test2.hour < 23)\n",
    "                         & (test2.weekday > 0) & (test2.weekday < 7))]\n",
    "        test2_order = test2.sort_values(by=['power'], ascending=False)['power']\n",
    "        metric = pd.Series(index=['Annual Electric Consumption',\n",
    "                                  'Peak Electric Demand Summer',\n",
    "                                  'Peak Electric Demand Winter',\n",
    "                                  'Demand Threshold at the Top 50 Hours',\n",
    "                                  'Annual Average Electric Energy Intensity',\n",
    "                                  'Annual Peak Electric Demand Intensity',\n",
    "                                  'Weather Sensitivity Occupied',\n",
    "                                  'Weather Sensitivity Unoccupied'])\n",
    "        metric['Annual Electric Consumption'] = test2.power.sum()\n",
    "        metric['Peak Electric Demand Summer'] = test2[test2.month.isin(\n",
    "            [6, 7, 8])].power.max()\n",
    "        metric['Peak Electric Demand Winter'] = test2[test2.month.isin(\n",
    "            [12, 1, 2])].power.max()\n",
    "        metric['Annual Average Electric Energy Intensity'] = metric['Annual Electric Consumption'] / float(\n",
    "            self.site['floor_area'])\n",
    "        metric['Annual Peak Electric Demand Intensity'] = test2.power.max(\n",
    "        ) * 1000 / float(self.site['floor_area'])\n",
    "        metric['Weather Sensitivity Occupied'] = round(\n",
    "            df_wk[['oat', 'power']].corr()['power'][0], 2)\n",
    "        metric['Weather Sensitivity Unoccupied'] = round(\n",
    "            df_wkd[['oat', 'power']].corr()['power'][0], 2)\n",
    "        metric['Demand Threshold at the Top 50 Hours'] = test2_order[50]\n",
    "        metric.to_csv(self.out_dir.joinpath(\n",
    "            '{}_load-metric.csv'.format(self.site['site_id'])))\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Load Metrics\n",
    "Using the `calc_load_stats` method from the `SiteData` class, we compute the following load metrics:\n",
    "1. Annual Electric Consumption\n",
    "2. Peak Electric Demand (Summer/Winter)\n",
    "3. Demand Threshold at the Top 50 Hours\n",
    "4. Energy Intensity Metrics\n",
    "5. Weather Sensitivity (Occupied/Unoccupied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the preprocessed meter and weather data.\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/6vcnqd4d3nlcwtdmsgb31hx40000gn/T/ipykernel_52467/808833261.py:114: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_wk[['oat', 'power']].corr()['power'][0], 2)\n",
      "/var/folders/fq/6vcnqd4d3nlcwtdmsgb31hx40000gn/T/ipykernel_52467/808833261.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_wkd[['oat', 'power']].corr()['power'][0], 2)\n",
      "/var/folders/fq/6vcnqd4d3nlcwtdmsgb31hx40000gn/T/ipykernel_52467/808833261.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  metric['Demand Threshold at the Top 50 Hours'] = test2_order[50]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the SiteData class\n",
    "site = {\n",
    "    'site_id': 'TwoCarnegiePlaza',\n",
    "    'floor_area': 50000  # Example floor area in square feet\n",
    "}\n",
    "site_data = SiteData(site)\n",
    "metric = site_data.calc_load_stats()\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Baseline Models\n",
    "We will use the `BaselineModels` class to:\n",
    "1. Calculate event-day baselines using historical data.\n",
    "2. Assess the performance of regression models for predicting energy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample DR event\n",
    "event = {\n",
    "    'date': '2008-07-09',\n",
    "    'shed_start_time': '7/9/08 14:00',\n",
    "    'shed_end_time': '7/9/08 18:00'\n",
    "}\n",
    "\n",
    "# Initialize BaselineModels\n",
    "baseline_model = BaselineModels(site, meter_data, event)\n",
    "\n",
    "# Example: Calculate matching baseline\n",
    "dr_data = baseline_model.calc_match_baseline(x=5, y=10)\n",
    "print(dr_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Regression Modeling\n",
    "We fit a regression model using outdoor air temperature (OAT) and hour as predictors for energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression model\n",
    "model = baseline_model.oat_reg_model(meter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Performance Metrics\n",
    "Evaluate the regression model's performance using metrics such as:\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-Squared (R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metrics calculation\n",
    "y_true = meter_data['power']\n",
    "y_pred = model.predict(meter_data[['oat', 'hour']])\n",
    "metrics = calc_model_metrics(y_true, y_pred, num_predictor=2)\n",
    "\n",
    "print(\"Model Metrics:\")\n",
    "print(f\"MAE: {metrics[0]}\")\n",
    "print(f\"RMSE: {metrics[4]}\")\n",
    "print(f\"R²: {metrics[6]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
